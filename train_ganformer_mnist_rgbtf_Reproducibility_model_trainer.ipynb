{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rocks2021/training-gans/blob/main/train_ganformer_mnist_rgbtf_Reproducibility_model_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "6wnazjNboPho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43497348-bf9f-4535-c9c3-b449ae74f15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May  8 20:40:52 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtRrqpGPoZP8",
        "outputId": "80fce19c-fba9-4fd4-eed8-e5a3eb686985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hj7Knynoh3z",
        "outputId": "c08e673b-e445-4455-ae20-91286c9cf10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF_Ce0gGyfaP",
        "outputId": "a3b1a568-3c16-435b-c020-9fd3acb88769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'gansformer-reproducibility-challenge' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/GiorgiaAuroraAdorni/gansformer-reproducibility-challenge.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzDuIoMcqfBT",
        "outputId": "d9df2e7f-2660-4a19-8fde-8752f7087a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Tensorflow version: 1.15.2\n",
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-36e2419d-af0f-5332-69ec-7b00ed649a23)\n",
            "GPU Identified at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import gdown\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "print('Tensorflow version: {}'.format(tf.__version__) )\n",
        "!nvidia-smi -L\n",
        "print('GPU Identified at: {}'.format(tf.test.gpu_device_name()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.19.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "HMc67YcSsadE",
        "outputId": "f262a283-af83-4df4-91be-8e7887fa70e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 4.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lucid 0.3.10 requires umap-learn, which is not installed.\n",
            "tensorflow 1.15.2 requires gast==0.2.2, but you have gast 0.5.3 which is incompatible.\n",
            "lucid 0.3.10 requires numpy<=1.19, but you have numpy 1.19.5 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.19.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEMscuV6qk9u",
        "outputId": "92f216e0-b27a-4687-c942-9c494c11c442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/gansformer-reproducibility-challenge/src/training\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/gansformer-reproducibility-challenge/src/training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSVoMnq1jNE-",
        "outputId": "8ca8b647-e45d-45e9-ddca-bfa0ea2c435a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting training_loop.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile training_loop.py\n",
        "\n",
        "# Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\n",
        "#\n",
        "# This work is made available under the Nvidia Source Code License-NC.\n",
        "# To view a copy of this license, visit\n",
        "# https://nvlabs.github.io/stylegan2/license.html\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "from dnnlib.tflib.autosummary import autosummary\n",
        "\n",
        "from training import dataset\n",
        "from training import misc\n",
        "from metrics import metric_base\n",
        "from metrics.metric_defaults import metric_defaults\n",
        "import gc\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Just-in-time processing of training images before feeding them to the networks.\n",
        "\n",
        "def process_reals(x, labels, lod, mirror_augment, drange_data, drange_net):\n",
        "    with tf.name_scope('DynamicRange'):\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        x = misc.adjust_dynamic_range(x, drange_data, drange_net)\n",
        "    if mirror_augment:\n",
        "        with tf.name_scope('MirrorAugment'):\n",
        "            x = tf.where(tf.random_uniform([tf.shape(x)[0]]) < 0.5, x, tf.reverse(x, [3]))\n",
        "    with tf.name_scope('FadeLOD'): # Smooth crossfade between consecutive levels-of-detail.\n",
        "        s = tf.shape(x)\n",
        "        y = tf.reshape(x, [-1, s[1], s[2]//2, 2, s[3]//2, 2])\n",
        "        y = tf.reduce_mean(y, axis=[3, 5], keepdims=True)\n",
        "        y = tf.tile(y, [1, 1, 1, 2, 1, 2])\n",
        "        y = tf.reshape(y, [-1, s[1], s[2], s[3]])\n",
        "        x = tflib.lerp(x, y, lod - tf.floor(lod))\n",
        "    with tf.name_scope('UpscaleLOD'): # Upscale to match the expected input/output size of the networks.\n",
        "        s = tf.shape(x)\n",
        "        factor = tf.cast(2 ** tf.floor(lod), tf.int32)\n",
        "        x = tf.reshape(x, [-1, s[1], s[2], 1, s[3], 1])\n",
        "        x = tf.tile(x, [1, 1, 1, factor, 1, factor])\n",
        "        x = tf.reshape(x, [-1, s[1], s[2] * factor, s[3] * factor])\n",
        "    return x, labels\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Evaluate time-varying training parameters.\n",
        "\n",
        "def training_schedule(\n",
        "    cur_nimg,\n",
        "    training_set,\n",
        "    lod_initial_resolution  = None,     # Image resolution used at the beginning.\n",
        "    lod_training_kimg       = 600,      # Thousands of real images to show before doubling the resolution.\n",
        "    lod_transition_kimg     = 600,      # Thousands of real images to show when fading in new layers.\n",
        "    minibatch_size_base     = 16,       # Global minibatch size.\n",
        "    minibatch_size_dict     = {},       # Resolution-specific overrides.\n",
        "    minibatch_gpu_base      = 4,        # Number of samples processed at a time by one GPU.\n",
        "    minibatch_gpu_dict      = {},       # Resolution-specific overrides.\n",
        "    G_lrate_base            = 0.002,    # Learning rate for the generator.\n",
        "    G_lrate_dict            = {},       # Resolution-specific overrides.\n",
        "    D_lrate_base            = 0.002,    # Learning rate for the discriminator.\n",
        "    D_lrate_dict            = {},       # Resolution-specific overrides.\n",
        "    lrate_rampup_kimg       = 0,        # Duration of learning rate ramp-up.\n",
        "    tick_kimg_base          = 160,        # Default interval of progress snapshots.\n",
        "    tick_size          = 16): # Resolution-specific overrides.\n",
        "\n",
        "    # Initialize result dict.\n",
        "    s = dnnlib.EasyDict()\n",
        "    s.kimg = cur_nimg / 1000.0\n",
        "\n",
        "    # Training phase.\n",
        "    phase_dur = lod_training_kimg + lod_transition_kimg\n",
        "    phase_idx = int(np.floor(s.kimg / phase_dur)) if phase_dur > 0 else 0\n",
        "    phase_kimg = s.kimg - phase_idx * phase_dur\n",
        "    # Level-of-detail and resolution.\n",
        "    if lod_initial_resolution is None:\n",
        "        s.lod = 0.0\n",
        "    else:\n",
        "        s.lod = training_set.resolution_log2\n",
        "        s.lod -= np.floor(np.log2(lod_initial_resolution))\n",
        "        s.lod -= phase_idx\n",
        "        if lod_transition_kimg > 0:\n",
        "            s.lod -= max(phase_kimg - lod_training_kimg, 0.0) / lod_transition_kimg\n",
        "        s.lod = max(s.lod, 0.0)\n",
        "    s.resolution = 2 ** (training_set.resolution_log2 - int(np.floor(s.lod)))\n",
        "\n",
        "    # Minibatch size.\n",
        "    s.minibatch_size = minibatch_size_dict.get(s.resolution, minibatch_size_base)\n",
        "    s.minibatch_gpu = minibatch_gpu_dict.get(s.resolution, minibatch_gpu_base)\n",
        "\n",
        "    # Learning rate.\n",
        "    s.G_lrate = G_lrate_dict.get(s.resolution, G_lrate_base)\n",
        "    s.D_lrate = D_lrate_dict.get(s.resolution, D_lrate_base)\n",
        "    if lrate_rampup_kimg > 0:\n",
        "        rampup = min(s.kimg / lrate_rampup_kimg, 1.0)\n",
        "        s.G_lrate *= rampup\n",
        "        s.D_lrate *= rampup\n",
        "\n",
        "    # Other parameters.\n",
        "    s.tick_kimg = tick_size\n",
        "    return s\n",
        "\n",
        "\n",
        "\n",
        "def training_loop(\n",
        "    G_args                  = {},       # Options for generator network.\n",
        "    D_args                  = {},       # Options for discriminator network.\n",
        "    G_opt_args              = {},       # Options for generator optimizer.\n",
        "    D_opt_args              = {},       # Options for discriminator optimizer.\n",
        "    G_loss_args             = {},       # Options for generator loss.\n",
        "    D_loss_args             = {},       # Options for discriminator loss.\n",
        "    dataset_args            = {},       # Options for dataset.load_dataset().\n",
        "    sched_args              = {},       # Options for train.TrainingSchedule.\n",
        "    grid_args               = {},       # Options for train.setup_snapshot_image_grid().\n",
        "    metric_arg_list         = [],       # Options for MetricGroup.\n",
        "    metrics_10k_arg_list    = [],       # Options for MetricGroup_10k.\n",
        "    tf_config               = {},       # Options for tflib.init_tf().\n",
        "    data_dir                = None,     # Directory to load datasets from.\n",
        "    G_smoothing_kimg        = 10.0,     # Half-life of the running average of generator weights.\n",
        "    minibatch_repeats       = 4,        # Number of minibatches to run before adjusting training parameters.\n",
        "    lazy_regularization     = True,     # Perform regularization as a separate training step?\n",
        "    G_reg_interval          = 4,        # How often the perform regularization for G? Ignored if lazy_regularization=False.\n",
        "    D_reg_interval          = 16,       # How often the perform regularization for D? Ignored if lazy_regularization=False.\n",
        "    reset_opt_for_new_lod   = True,     # Reset optimizer internal state (e.g. Adam moments) when new layers are introduced?\n",
        "    total_kimg              = 25000,    # Total length of the training, measured in thousands of real images.\n",
        "    mirror_augment          = False,    # Enable mirror augment?\n",
        "    drange_net              = [-1,1],   # Dynamic range used when feeding image data to the networks.\n",
        "    image_snapshot_ticks    = 1,       # How often to save image snapshots? None = only save 'reals.png' and 'fakes-init.png'.\n",
        "    network_snapshot_ticks  = 1,       # How often to save network snapshots? None = only save 'networks-final.pkl'.\n",
        "    save_tf_graph           = False,    # Include full TensorFlow computation graph in the tfevents file?\n",
        "    save_weight_histograms  = False,    # Include weight histograms in the tfevents file?\n",
        "    resume_pkl              = None,     # Network pickle to resume training from, None = train from scratch.\n",
        "    resume_kimg             = 0,      # Assumed training progress at the beginning. Affects reporting and training schedule.\n",
        "    resume_time             = 0.0,      # Assumed wallclock time at the beginning. Affects reporting.\n",
        "    resume_with_new_nets    = False,\n",
        "    tick_size               = 16,\n",
        "    snapshot_saving_size    = 2,        # Number of snapshots to save\n",
        "    ganformer=True):   # Construct new networks according to G_args and D_args before resuming training?\n",
        "\n",
        "    # Initialize dnnlib and TensorFlow.\n",
        "    tflib.init_tf(tf_config)\n",
        "    num_gpus = dnnlib.submit_config.num_gpus\n",
        "\n",
        "    # Load training set.\n",
        "    training_set = dataset.load_dataset(data_dir=dnnlib.convert_path(data_dir), verbose=True, **dataset_args)\n",
        "    grid_size, grid_reals, grid_labels = misc.setup_snapshot_image_grid(training_set, **grid_args)\n",
        "    misc.save_image_grid(grid_reals, dnnlib.make_run_dir_path('reals.png'), drange=training_set.dynamic_range, grid_size=grid_size)\n",
        "\n",
        "    # Construct or load networks.\n",
        "    with tf.device('/gpu:0'):\n",
        "        if resume_pkl is None or resume_with_new_nets:\n",
        "            print('Constructing networks...')\n",
        "            G = tflib.Network('G', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **G_args)\n",
        "            D = tflib.Network('D', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **D_args)\n",
        "            Gs = G.clone('Gs')\n",
        "        if resume_pkl is not None:\n",
        "            print('Loading networks from \"%s\"...' % resume_pkl)\n",
        "            rG, rD, rGs = misc.load_pkl(resume_pkl)\n",
        "            if resume_with_new_nets: G.copy_vars_from(rG); D.copy_vars_from(rD); Gs.copy_vars_from(rGs)\n",
        "            else: G = rG; D = rD; Gs = rGs\n",
        "\n",
        "    # Print layers and generate initial image snapshot.\n",
        "    G.print_layers(); D.print_layers()\n",
        "    sched = training_schedule(cur_nimg=total_kimg*1000, training_set=training_set, tick_size=tick_size, **sched_args)\n",
        "    grid_latents = np.random.randn(np.prod(grid_size), *G.input_shape[1:])\n",
        "    grid_fakes = Gs.run(grid_latents, grid_labels, is_validation=True, minibatch_size=sched.minibatch_gpu)\n",
        "    grid=grid_fakes\n",
        "    if ganformer:\n",
        "      grid=grid_fakes[0]\n",
        "    misc.save_image_grid(grid, dnnlib.make_run_dir_path('fakes_init.png'), drange=drange_net, grid_size=grid_size)\n",
        "\n",
        "    # Setup training inputs.\n",
        "    print('Building TensorFlow graph...')\n",
        "    with tf.name_scope('Inputs'), tf.device('/cpu:0'):\n",
        "        lod_in               = tf.placeholder(tf.float32, name='lod_in', shape=[])\n",
        "        lrate_in             = tf.placeholder(tf.float32, name='lrate_in', shape=[])\n",
        "        minibatch_size_in    = tf.placeholder(tf.int32, name='minibatch_size_in', shape=[])\n",
        "        minibatch_gpu_in     = tf.placeholder(tf.int32, name='minibatch_gpu_in', shape=[])\n",
        "        minibatch_multiplier = minibatch_size_in // (minibatch_gpu_in * num_gpus)\n",
        "        Gs_beta              = 0.5 ** tf.div(tf.cast(minibatch_size_in, tf.float32), G_smoothing_kimg * 1000.0) if G_smoothing_kimg > 0.0 else 0.0\n",
        "\n",
        "    # Setup optimizers.\n",
        "    G_opt_args = dict(G_opt_args)\n",
        "    D_opt_args = dict(D_opt_args)\n",
        "    for args, reg_interval in [(G_opt_args, G_reg_interval), (D_opt_args, D_reg_interval)]:\n",
        "        args['minibatch_multiplier'] = minibatch_multiplier\n",
        "        args['learning_rate'] = lrate_in\n",
        "        if lazy_regularization:\n",
        "            mb_ratio = reg_interval / (reg_interval + 1)\n",
        "            args['learning_rate'] *= mb_ratio\n",
        "            if 'beta1' in args: args['beta1'] **= mb_ratio\n",
        "            if 'beta2' in args: args['beta2'] **= mb_ratio\n",
        "    G_opt = tflib.Optimizer(name='TrainG', **G_opt_args)\n",
        "    D_opt = tflib.Optimizer(name='TrainD', **D_opt_args)\n",
        "    G_reg_opt = tflib.Optimizer(name='RegG', share=G_opt, **G_opt_args)\n",
        "    D_reg_opt = tflib.Optimizer(name='RegD', share=D_opt, **D_opt_args)\n",
        "\n",
        "    # Build training graph for each GPU.\n",
        "    data_fetch_ops = []\n",
        "    for gpu in range(num_gpus):\n",
        "        with tf.name_scope('GPU%d' % gpu), tf.device('/gpu:%d' % gpu):\n",
        "\n",
        "            # Create GPU-specific shadow copies of G and D.\n",
        "            G_gpu = G if gpu == 0 else G.clone(G.name + '_shadow')\n",
        "            D_gpu = D if gpu == 0 else D.clone(D.name + '_shadow')\n",
        "\n",
        "            # Fetch training data via temporary variables.\n",
        "            with tf.name_scope('DataFetch'):\n",
        "                sched = training_schedule(cur_nimg=int(resume_kimg*1000), training_set=training_set,tick_size=tick_size, **sched_args)\n",
        "                reals_var = tf.Variable(name='reals', trainable=False, initial_value=tf.zeros([sched.minibatch_gpu] + training_set.shape))\n",
        "                labels_var = tf.Variable(name='labels', trainable=False, initial_value=tf.zeros([sched.minibatch_gpu, training_set.label_size]))\n",
        "                reals_write, labels_write = training_set.get_minibatch_tf()\n",
        "                reals_write, labels_write = process_reals(reals_write, labels_write, lod_in, mirror_augment, training_set.dynamic_range, drange_net)\n",
        "                reals_write = tf.concat([reals_write, reals_var[minibatch_gpu_in:]], axis=0)\n",
        "                labels_write = tf.concat([labels_write, labels_var[minibatch_gpu_in:]], axis=0)\n",
        "                data_fetch_ops += [tf.assign(reals_var, reals_write)]\n",
        "                data_fetch_ops += [tf.assign(labels_var, labels_write)]\n",
        "                reals_read = reals_var[:minibatch_gpu_in]\n",
        "                labels_read = labels_var[:minibatch_gpu_in]\n",
        "\n",
        "            # Evaluate loss functions.\n",
        "            lod_assign_ops = []\n",
        "            if 'lod' in G_gpu.vars: lod_assign_ops += [tf.assign(G_gpu.vars['lod'], lod_in)]\n",
        "            if 'lod' in D_gpu.vars: lod_assign_ops += [tf.assign(D_gpu.vars['lod'], lod_in)]\n",
        "            with tf.control_dependencies(lod_assign_ops):\n",
        "                with tf.name_scope('G_loss'):\n",
        "                    G_loss, G_reg = dnnlib.util.call_func_by_name(G=G_gpu, D=D_gpu, opt=G_opt, training_set=training_set, minibatch_size=minibatch_gpu_in, **G_loss_args)\n",
        "                with tf.name_scope('D_loss'):\n",
        "                    D_loss, D_reg = dnnlib.util.call_func_by_name(G=G_gpu, D=D_gpu, opt=D_opt, training_set=training_set, minibatch_size=minibatch_gpu_in, reals=reals_read, labels=labels_read, **D_loss_args)\n",
        "\n",
        "            # Register gradients.\n",
        "            if not lazy_regularization:\n",
        "                if G_reg is not None: G_loss += G_reg\n",
        "                if D_reg is not None: D_loss += D_reg\n",
        "            else:\n",
        "                if G_reg is not None: G_reg_opt.register_gradients(tf.reduce_mean(G_reg * G_reg_interval), G_gpu.trainables)\n",
        "                if D_reg is not None: D_reg_opt.register_gradients(tf.reduce_mean(D_reg * D_reg_interval), D_gpu.trainables)\n",
        "            G_opt.register_gradients(tf.reduce_mean(G_loss), G_gpu.trainables)\n",
        "            D_opt.register_gradients(tf.reduce_mean(D_loss), D_gpu.trainables)\n",
        "\n",
        "    # Setup training ops.\n",
        "    data_fetch_op = tf.group(*data_fetch_ops)\n",
        "    G_train_op = G_opt.apply_updates()\n",
        "    D_train_op = D_opt.apply_updates()\n",
        "    G_reg_op = G_reg_opt.apply_updates(allow_no_op=True)\n",
        "    D_reg_op = D_reg_opt.apply_updates(allow_no_op=True)\n",
        "    Gs_update_op = Gs.setup_as_moving_average_of(G, beta=Gs_beta)\n",
        "\n",
        "    # Finalize graph.\n",
        "    with tf.device('/gpu:0'):\n",
        "        try:\n",
        "            peak_gpu_mem_op = tf.contrib.memory_stats.MaxBytesInUse()\n",
        "        except tf.errors.NotFoundError:\n",
        "            peak_gpu_mem_op = tf.constant(0)\n",
        "    tflib.init_uninitialized_vars()\n",
        "\n",
        "    print('Initializing logs...')\n",
        "    summary_log = tf.summary.FileWriter(dnnlib.make_run_dir_path())\n",
        "    if save_tf_graph:\n",
        "        summary_log.add_graph(tf.get_default_graph())\n",
        "    if save_weight_histograms:\n",
        "        G.setup_weight_histograms(); D.setup_weight_histograms()\n",
        "    metrics = metric_base.MetricGroup(metric_arg_list)\n",
        "    metrics_10k = metric_base.MetricGroup(metrics_10k_arg_list)\n",
        "\n",
        "    print('Training for %d kimg...\\n' % total_kimg)\n",
        "    dnnlib.RunContext.get().update('', cur_epoch=resume_kimg, max_epoch=total_kimg)\n",
        "    maintenance_time = dnnlib.RunContext.get().get_last_update_interval()\n",
        "    cur_nimg = int(resume_kimg * 1000)\n",
        "    cur_tick = -1\n",
        "    tick_start_nimg = cur_nimg\n",
        "    prev_lod = -1.0\n",
        "    running_mb_counter = 0\n",
        "    while cur_nimg < total_kimg * 1000:\n",
        "        if dnnlib.RunContext.get().should_stop(): break\n",
        "\n",
        "        # Choose training parameters and configure training ops.\n",
        "        sched = training_schedule(cur_nimg=cur_nimg, training_set=training_set,tick_size=tick_size, **sched_args)\n",
        "        assert sched.minibatch_size % (sched.minibatch_gpu * num_gpus) == 0\n",
        "        training_set.configure(sched.minibatch_gpu, sched.lod)\n",
        "        if reset_opt_for_new_lod:\n",
        "            if np.floor(sched.lod) != np.floor(prev_lod) or np.ceil(sched.lod) != np.ceil(prev_lod):\n",
        "                G_opt.reset_optimizer_state(); D_opt.reset_optimizer_state()\n",
        "        prev_lod = sched.lod\n",
        "\n",
        "        # Run training ops.\n",
        "        feed_dict = {lod_in: sched.lod, lrate_in: sched.G_lrate, minibatch_size_in: sched.minibatch_size, minibatch_gpu_in: sched.minibatch_gpu}\n",
        "        for _repeat in range(minibatch_repeats):\n",
        "            rounds = range(0, sched.minibatch_size, sched.minibatch_gpu * num_gpus)\n",
        "            run_G_reg = (lazy_regularization and running_mb_counter % G_reg_interval == 0)\n",
        "            run_D_reg = (lazy_regularization and running_mb_counter % D_reg_interval == 0)\n",
        "            cur_nimg += sched.minibatch_size\n",
        "            running_mb_counter += 1\n",
        "\n",
        "            # Fast path without gradient accumulation.\n",
        "            if len(rounds) == 1:\n",
        "                tflib.run([G_train_op, data_fetch_op], feed_dict)\n",
        "                if run_G_reg:\n",
        "                    tflib.run(G_reg_op, feed_dict)\n",
        "                tflib.run([D_train_op, Gs_update_op], feed_dict)\n",
        "                if run_D_reg:\n",
        "                    tflib.run(D_reg_op, feed_dict)\n",
        "\n",
        "            # Slow path with gradient accumulation.\n",
        "            else:\n",
        "                for _round in rounds:\n",
        "                    tflib.run(G_train_op, feed_dict)\n",
        "                if run_G_reg:\n",
        "                    for _round in rounds:\n",
        "                        tflib.run(G_reg_op, feed_dict)\n",
        "                tflib.run(Gs_update_op, feed_dict)\n",
        "                for _round in rounds:\n",
        "                    tflib.run(data_fetch_op, feed_dict)\n",
        "                    tflib.run(D_train_op, feed_dict)\n",
        "                if run_D_reg:\n",
        "                    for _round in rounds:\n",
        "                        tflib.run(D_reg_op, feed_dict)\n",
        "\n",
        "        # Perform maintenance tasks once per tick.\n",
        "        done = (cur_nimg >= total_kimg * 1000)\n",
        "        if cur_tick < 0 or cur_nimg >= tick_start_nimg + sched.tick_kimg * 1000 or done:\n",
        "            cur_tick += 1\n",
        "            tick_kimg = (cur_nimg - tick_start_nimg) / 1000.0\n",
        "            tick_start_nimg = cur_nimg\n",
        "            tick_time = dnnlib.RunContext.get().get_time_since_last_update()\n",
        "            total_time = dnnlib.RunContext.get().get_time_since_start() + resume_time\n",
        "\n",
        "            # Report progress.\n",
        "            print('tick %-5d kimg %-8.1f lod %-5.2f minibatch %-4d time %-12s sec/tick %-7.1f sec/kimg %-7.2f maintenance %-6.1f gpumem %.1f' % (\n",
        "                autosummary('Progress/tick', cur_tick),\n",
        "                autosummary('Progress/kimg', cur_nimg / 1000.0),\n",
        "                autosummary('Progress/lod', sched.lod),\n",
        "                autosummary('Progress/minibatch', sched.minibatch_size),\n",
        "                dnnlib.util.format_time(autosummary('Timing/total_sec', total_time)),\n",
        "                autosummary('Timing/sec_per_tick', tick_time),\n",
        "                autosummary('Timing/sec_per_kimg', tick_time / tick_kimg),\n",
        "                autosummary('Timing/maintenance_sec', maintenance_time),\n",
        "                autosummary('Resources/peak_gpu_mem_gb', peak_gpu_mem_op.eval() / 2**30)))\n",
        "            autosummary('Timing/total_hours', total_time / (60.0 * 60.0))\n",
        "            autosummary('Timing/total_days', total_time / (24.0 * 60.0 * 60.0))\n",
        "\n",
        "            # Save snapshots.\n",
        "            if image_snapshot_ticks is not None and (cur_tick % image_snapshot_ticks == 0 or done):\n",
        "                grid_fakes = Gs.run(grid_latents, grid_labels, is_validation=True, minibatch_size=sched.minibatch_gpu)\n",
        "                grid=grid_fakes\n",
        "                if ganformer:\n",
        "                  grid=grid_fakes[0]\n",
        "                misc.save_image_grid(grid, dnnlib.make_run_dir_path('fakes%06d.png') % (cur_nimg // 1000), drange=drange_net, grid_size=grid_size)\n",
        "            if network_snapshot_ticks is not None and (cur_tick % network_snapshot_ticks == 0 or done):\n",
        "                pkl = dnnlib.make_run_dir_path('network-snapshot-%06d.pkl') % (cur_nimg // 1000)\n",
        "                misc.save_pkl((G, D, Gs), pkl)\n",
        "                metrics_10k.run(dnnlib.make_run_dir_path(('network-snapshot-%06d.pkl')% (cur_nimg // 1000)), run_dir=dnnlib.make_run_dir_path(), data_dir=dnnlib.convert_path(data_dir), num_gpus=num_gpus, tf_config=tf_config, ganformer=ganformer)\n",
        "                gc.collect(generation=2)\n",
        "            if snapshot_saving_size > 0:\n",
        "              files_to_remove = sorted(glob.glob(dnnlib.make_run_dir_path(\"network*.pkl\")))[:-snapshot_saving_size]\n",
        "              for f in files_to_remove:\n",
        "                os.remove(f)\n",
        "            # Update summaries and RunContext.\n",
        "            metrics.update_autosummaries()\n",
        "            tflib.autosummary.save_summaries(summary_log, cur_nimg)\n",
        "            dnnlib.RunContext.get().update('%.2f' % sched.lod, cur_epoch=cur_nimg // 1000, max_epoch=total_kimg)\n",
        "            maintenance_time = dnnlib.RunContext.get().get_last_update_interval() - tick_time\n",
        "\n",
        "    # Save final snapshot.\n",
        "    misc.save_pkl((G, D, Gs), dnnlib.make_run_dir_path('network-final.pkl'))\n",
        "    metrics.run(dnnlib.make_run_dir_path('network-final.pkl'), run_dir=dnnlib.make_run_dir_path(), data_dir=dnnlib.convert_path(data_dir), num_gpus=num_gpus, tf_config=tf_config, ganformer=ganformer)\n",
        "    # All done.\n",
        "    summary_log.close()\n",
        "    training_set.close()\n",
        "\n",
        "  #----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hhfj6YBxwAdb",
        "outputId": "e5035cbb-9377-443d-c681-691c26e8b451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/gansformer-reproducibility-challenge/src\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pufC7hNgpA_Y",
        "outputId": "a815e524-fd53-427b-d266-80695fde62ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_training.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile run_training.py\n",
        "# setup\n",
        "import argparse\n",
        "import copy\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import dnnlib\n",
        "from dnnlib import EasyDict\n",
        "import numpy as np\n",
        "from metrics.metric_defaults import metric_defaults\n",
        "\n",
        "\n",
        "ganformer=True\n",
        "duplex=False\n",
        "attention_discriminator=False\n",
        "tfrecords_dataset = 'mnist_rgbtf'\n",
        "img_resolution = 32 if tfrecords_dataset == 'mnist_rgbtf' else 128 #Resolution of the Image default 64 for Cartoon 128 for FFHQ\n",
        "base_2_log = int(np.log2(img_resolution))\n",
        "dataset = 'mnist_rgbtf'\n",
        "data_dir = '/content/drive/MyDrive/'\n",
        "# data_dir = '/content/datasets/'\n",
        "num_gpus = 1\n",
        "total_kimg = 1000\n",
        "mirror_augment = True\n",
        "metrics = ['fid50k', 'is50k','pr50k3'] #' \n",
        "metrics_10k = ['fid10k']\n",
        "gamma = None\n",
        "tick_size=16\n",
        "result_dir = '/content/drive/MyDrive/model'\n",
        "if ganformer:\n",
        "    result_dir = '/content/drive/MyDrive/GANFORMER_Duplex/' if duplex else '/content/drive/MyDrive/GANFORMER_Simplex/'\n",
        "else:\n",
        "    result_dir = '/content/drive/MyDrive/STYLEGAN2/'\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "train     = EasyDict(run_func_name='training.training_loop.training_loop') # Options for training loop.\n",
        "sched     = EasyDict()                                                     # Options for TrainingSchedule.\n",
        "grid      = EasyDict(size='1080p', layout='random')                           # Options for setup_snapshot_image_grid().\n",
        "sc        = dnnlib.SubmitConfig()                                          # Options for dnnlib.submit_run().\n",
        "tf_config = {'rnd.np_random_seed': 1000}                                   # Options for tflib.init_tf().\n",
        "\n",
        "\n",
        "if ganformer:\n",
        "  G = EasyDict(func_name='training.networks_GANFormer.G_GANformer', truncation_psi = 0.65, \n",
        "               architecture = 'resnet', latent_size = 32, \n",
        "               dlatent_size = 32, components_num = 16, \n",
        "               mapping_resnet = True, style = True, \n",
        "               fused_modconv = True, local_noise = True, \n",
        "               transformer = True, norm = 'layer', \n",
        "               integration = 'mul', kmeans = duplex, \n",
        "               kmeans_iters = 1, mapping_ltnt2ltnt = True, \n",
        "               use_pos = True, num_heads = 2, \n",
        "               pos_init = 'uniform', pos_directions_num = 2, \n",
        "               merge_layer = -1, start_res = 0, \n",
        "               end_res = base_2_log, img2img = 0, \n",
        "               style_mixing = 0.9, component_mixing = 0.0, \n",
        "               component_dropout = 0.0)       # Options for generator network.\n",
        "\n",
        "  if attention_discriminator:\n",
        "    func_name='training.networks_GANFormer.D_GANformer'\n",
        "  else:\n",
        "    func_name='training.networks_GANFormer.D_Stylegan'\n",
        "    \n",
        "  D = EasyDict(func_name=func_name, latent_size = 32,\n",
        "               components_num = 16, mbstd_group_size = 4, \n",
        "               use_pos = True, num_heads = 2, \n",
        "               pos_init = 'uniform', pos_directions_num = 2, \n",
        "               start_res = 0, end_res = base_2_log, img2img = 0)  # Options for discriminator network.\n",
        "  G_loss = EasyDict(func_name='training.loss.G_logistic_ns_pathreg')      # Options for generator loss.\n",
        "  D_loss = EasyDict(func_name='training.loss.D_logistic_r1')              # Options for discriminator loss.\n",
        "  # G_opt = EasyDict(beta1=0.9, beta2=0.999, epsilon=1e-3)                  # Options for generator optimizer.\n",
        "  # D_opt = EasyDict(beta1=0.9, beta2=0.999, epsilon=1e-3)                  # Options for discriminator optimizer.\n",
        "  G_opt = EasyDict(beta1=0.0, beta2=0.99, epsilon=1e-8)                  # Options for generator optimizer.\n",
        "  D_opt = EasyDict(beta1=0.0, beta2=0.99, epsilon=1e-8)                  # Options for discriminator optimizer.\n",
        "  desc = 'GANFormer'\n",
        "else:\n",
        "  G = EasyDict(func_name='training.networks_stylegan2.G_main')       # Options for generator network.\n",
        "  D = EasyDict(func_name='training.networks_stylegan2.D_stylegan2')  # Options for discriminator network.\n",
        "  G_loss = EasyDict(func_name='training.loss_stylegan2.G_logistic_ns_pathreg')      # Options for generator loss.\n",
        "  D_loss = EasyDict(func_name='training.loss_stylegan2.D_logistic_r1')              # Options for discriminator loss.\n",
        "  G_opt = EasyDict(beta1=0.0, beta2=0.99, epsilon=1e-8)                  # Options for generator optimizer.\n",
        "  D_opt = EasyDict(beta1=0.0, beta2=0.99, epsilon=1e-8)                  # Options for discriminator optimizer.\n",
        "  desc = 'stylegan2'\n",
        "\n",
        "\n",
        "\n",
        "train.data_dir = data_dir\n",
        "train.total_kimg = total_kimg\n",
        "train.mirror_augment = mirror_augment\n",
        "train.image_snapshot_ticks = train.network_snapshot_ticks = 10\n",
        "sched.G_lrate_base = sched.D_lrate_base = 0.002\n",
        "sched.minibatch_size_base = 24\n",
        "sched.minibatch_gpu_base = 12\n",
        "D_loss.gamma = 10\n",
        "metrics = [metric_defaults[x] for x in metrics]\n",
        "metrics_10k = [metric_defaults[x] for x in metrics_10k]\n",
        "\n",
        "\n",
        "desc += '-' + dataset\n",
        "dataset_args = EasyDict(tfrecord_dir=dataset, resolution=img_resolution)\n",
        "\n",
        "assert num_gpus in [1, 2, 4, 8]\n",
        "sc.num_gpus = num_gpus\n",
        "desc += '-%dgpu' % num_gpus\n",
        "\n",
        "if gamma is not None:\n",
        "    D_loss.gamma = gamma\n",
        "\n",
        "sc.submit_target = dnnlib.SubmitTarget.LOCAL\n",
        "sc.local.do_not_copy_source_files = True\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "kwargs = EasyDict(train)\n",
        "kwargs.update(G_args=G, D_args=D, G_opt_args=G_opt, D_opt_args=D_opt, G_loss_args=G_loss, D_loss_args=D_loss)\n",
        "kwargs.update(dataset_args=dataset_args, sched_args=sched, grid_args=grid, metric_arg_list=metrics, metrics_10k_arg_list=metrics_10k, tf_config=tf_config,tick_size=tick_size, ganformer=ganformer)\n",
        "kwargs.submit_config = copy.deepcopy(sc)\n",
        "kwargs.submit_config.run_dir_root = result_dir\n",
        "kwargs.submit_config.run_desc = desc\n",
        "dnnlib.submit_run(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwzV3TjcM36K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f773dbc-6601-4f67-f8d1-f8d0edfaec87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local submit - run_dir: /content/drive/MyDrive/GANFORMER_Simplex/00008-GANFormer-mnist_rgbtf-1gpu\n",
            "dnnlib: Running training.training_loop.training_loop() on localhost...\n",
            "Streaming data using training.dataset.TFRecordDataset...\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x87ae000 @  0x7f4be96b4001 0x7f4be619954f 0x7f4be61e9b58 0x7f4be61edb17 0x7f4be628c203 0x593835 0x548c51 0x5127f1 0x549e0e 0x4bca8a 0x532b86 0x53786a 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x5134a6 0x549576 0x4bca8a 0x5134a6 0x593dd7 0x5118f8 0x593dd7 0x511e2c 0x549576 0x4bca8a 0x5134a6 0x549576 0x604173 0x5f5506 0x5f8c6c\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f49f382a000 @  0x7f4be96b21e7 0x7f4be619946e 0x7f4be61e9c7b 0x7f4be61ea35f 0x7f4be628c103 0x593835 0x548c51 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x593dd7 0x5118f8 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x549e0e\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f48f2828000 @  0x7f4be96b21e7 0x7f4be619946e 0x7f4be61e9c7b 0x7f4be61ea35f 0x7f4b91164235 0x7f4b90ae7792 0x7f4b90ae7d42 0x7f4b90aa0aee 0x59371f 0x548c51 0x51566f 0x593dd7 0x511e2c 0x549e0e 0x4bcb19 0x5134a6 0x549576 0x593fce 0x511e2c 0x549e0e 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x4bcb19 0x59c019 0x595ef6 0x5134a6 0x549576 0x593fce\n",
            "Dataset shape = [3, 32, 32]\n",
            "Dynamic range = [0, 255]\n",
            "Label size    = 0\n",
            "Constructing networks...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Preprocessing... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Preprocessing... Loading... Done.\n",
            "\n",
            "G                                  Params    OutputShape       WeightShape     \n",
            "---                                ---       ---               ---             \n",
            "ltnt_emb/emb                               512       (16, 32)          (16, 32)        \n",
            "G_mapping/AttLayer_0                       6336      (?, 32)           -               \n",
            "G_mapping/Dense0_0                         1056      (?, 32)           (32, 32)        \n",
            "G_mapping/Dense0_1                         1056      (?, 32)           (32, 32)        \n",
            "G_mapping/AttLayer_1                       6336      (?, 32)           -               \n",
            "G_mapping/Dense1_0                         1056      (?, 32)           (32, 32)        \n",
            "G_mapping/Dense1_1                         1056      (?, 32)           (32, 32)        \n",
            "G_mapping/AttLayer_2                       6336      (?, 32)           -               \n",
            "G_mapping/Dense2_0                         1056      (?, 32)           (32, 32)        \n",
            "G_mapping/Dense2_1                         1056      (?, 32)           (32, 32)        \n",
            "G_mapping/AttLayer_3                       6336      (?, 32)           -               \n",
            "G_mapping/Dense3_0                         1056      (?, 32)           (32, 32)        \n",
            "G_mapping/Dense3_1                         1056      (?, 32)           (32, 32)        \n",
            "G_mapping/Dense3                           1056      (?, 32)           (32, 32)        \n",
            "G_mapping/global                           9504      (?, 32)           (32, 32)        \n",
            "G_synthesis/4x4/Const                      8192      (?, 512, 4, 4)    (1, 512, 4, 4)  \n",
            "G_synthesis/4x4/Conv/AttLayer_l2n          592896    (?, 512)          -               \n",
            "G_synthesis/4x4/Conv/weight                2359296   (3, 3, 512, 512)  (3, 3, 512, 512)\n",
            "G_synthesis/4x4/Conv/mod_weight            16384     (32, 512)         (32, 512)       \n",
            "G_synthesis/4x4/Conv/mod_bias              512       (512,)            (512,)          \n",
            "G_synthesis/4x4/Conv/noise_strength        1         ()                ()              \n",
            "G_synthesis/4x4/Conv/bias                  512       (512,)            (512,)          \n",
            "G_synthesis/8x8/Conv0_up/AttLayer_l2n      592896    (?, 512)          -               \n",
            "G_synthesis/8x8/Conv0_up/weight            2359296   (3, 3, 512, 512)  (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Conv0_up/mod_weight        16384     (32, 512)         (32, 512)       \n",
            "G_synthesis/8x8/Conv0_up/mod_bias          512       (512,)            (512,)          \n",
            "G_synthesis/8x8/Conv0_up/noise_strength    1         ()                ()              \n",
            "G_synthesis/8x8/Conv0_up/bias              512       (512,)            (512,)          \n",
            "G_synthesis/8x8/Conv1/AttLayer_l2n         592896    (?, 512)          -               \n",
            "G_synthesis/8x8/Conv1/weight               2359296   (3, 3, 512, 512)  (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Conv1/mod_weight           16384     (32, 512)         (32, 512)       \n",
            "G_synthesis/8x8/Conv1/mod_bias             512       (512,)            (512,)          \n",
            "G_synthesis/8x8/Conv1/noise_strength       1         ()                ()              \n",
            "G_synthesis/8x8/Conv1/bias                 512       (512,)            (512,)          \n",
            "G_synthesis/8x8/Skip                       262144    (?, 512, 8, 8)    (1, 1, 512, 512)\n",
            "G_synthesis/16x16/Conv0_up/AttLayer_l2n    592896    (?, 512)          -               \n",
            "G_synthesis/16x16/Conv0_up/weight          2359296   (3, 3, 512, 512)  (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Conv0_up/mod_weight      16384     (32, 512)         (32, 512)       \n",
            "G_synthesis/16x16/Conv0_up/mod_bias        512       (512,)            (512,)          \n",
            "G_synthesis/16x16/Conv0_up/noise_strength  1         ()                ()              \n",
            "G_synthesis/16x16/Conv0_up/bias            512       (512,)            (512,)          \n",
            "G_synthesis/16x16/Conv1/AttLayer_l2n       592896    (?, 512)          -               \n",
            "G_synthesis/16x16/Conv1/weight             2359296   (3, 3, 512, 512)  (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Conv1/mod_weight         16384     (32, 512)         (32, 512)       \n",
            "G_synthesis/16x16/Conv1/mod_bias           512       (512,)            (512,)          \n",
            "G_synthesis/16x16/Conv1/noise_strength     1         ()                ()              \n",
            "G_synthesis/16x16/Conv1/bias               512       (512,)            (512,)          \n",
            "G_synthesis/16x16/Skip                     262144    (?, 512, 16, 16)  (1, 1, 512, 512)\n",
            "G_synthesis/32x32                          7410181   (?, 3, 32, 32)    (1, 1, 512, 512)\n",
            "---                                ---       ---               ---             \n",
            "Total                              22835530                                    \n",
            "\n",
            "\n",
            "D         Params    OutputShape       WeightShape     \n",
            "---       ---       ---               ---             \n",
            "32x32/FromRGB     2048      (?, 512, 32, 32)  (1, 1, 3, 512)  \n",
            "32x32/Conv0       2359808   (?, 512, 32, 32)  (3, 3, 512, 512)\n",
            "32x32/Conv1_down  2359808   (?, 512, 16, 16)  (3, 3, 512, 512)\n",
            "32x32/Skip        262144    (?, 512, 16, 16)  (1, 1, 512, 512)\n",
            "16x16/Conv0       2359808   (?, 512, 16, 16)  (3, 3, 512, 512)\n",
            "16x16/Conv1_down  2359808   (?, 512, 8, 8)    (3, 3, 512, 512)\n",
            "16x16/Skip        262144    (?, 512, 8, 8)    (1, 1, 512, 512)\n",
            "8x8/Conv0         2359808   (?, 512, 8, 8)    (3, 3, 512, 512)\n",
            "8x8/Conv1_down    2359808   (?, 512, 4, 4)    (3, 3, 512, 512)\n",
            "8x8/Skip          262144    (?, 512, 4, 4)    (1, 1, 512, 512)\n",
            "4x4/Conv          2364416   (?, 512, 4, 4)    (3, 3, 513, 512)\n",
            "4x4/Dense0        4194816   (?, 512)          (8192, 512)     \n",
            "Output/weight     512       (512, 1)          (512, 1)        \n",
            "Output/bias       1         (1,)              (1,)            \n",
            "---       ---       ---               ---             \n",
            "Total     21507073                                    \n",
            "\n",
            "Building TensorFlow graph...\n",
            "Initializing logs...\n",
            "Training for 1000 kimg...\n",
            "\n",
            "tick 0     kimg 0.1      lod 0.00  minibatch 24   time 16s          sec/tick 16.4    sec/kimg 170.52  maintenance 0.0    gpumem 5.6\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f44472fe000 @  0x7f4be96b4001 0x7f4be619954f 0x7f4be61e9b58 0x7f4be61edb17 0x7f4be628c203 0x593835 0x548c51 0x5127f1 0x549e0e 0x4bca8a 0x532b86 0x53786a 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f4346afe000 @  0x7f4be96b21e7 0x7f4be619946e 0x7f4be61e9c7b 0x7f4be61ea35f 0x7f4be628c103 0x593835 0x548c51 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x593dd7 0x5118f8 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x549e0e\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f4346afe000 @  0x7f4be96b21e7 0x7f4be619946e 0x7f4be61e9c7b 0x7f4be61ea35f 0x7f4b91164235 0x7f4b90ae7792 0x7f4b90ae7d42 0x7f4b90aa0aee 0x59371f 0x548c51 0x51566f 0x593dd7 0x511e2c 0x549e0e 0x4bcb19 0x5134a6 0x549576 0x593fce 0x511e2c 0x549e0e 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x4bcb19 0x59c019 0x595ef6 0x5134a6 0x549576 0x593fce\n",
            "network-snapshot-000000        time 2m 48s       fid10k 243.1747\n",
            "tick 1     kimg 16.1     lod 0.00  minibatch 24   time 16m 35s      sec/tick 776.8   sec/kimg 48.45   maintenance 202.2  gpumem 5.6\n",
            "tick 2     kimg 32.2     lod 0.00  minibatch 24   time 29m 32s      sec/tick 776.4   sec/kimg 48.43   maintenance 0.0    gpumem 5.6\n",
            "tick 3     kimg 48.2     lod 0.00  minibatch 24   time 42m 28s      sec/tick 775.9   sec/kimg 48.40   maintenance 0.0    gpumem 5.6\n",
            "tick 4     kimg 64.2     lod 0.00  minibatch 24   time 55m 23s      sec/tick 775.4   sec/kimg 48.37   maintenance 0.0    gpumem 5.6\n",
            "tick 5     kimg 80.3     lod 0.00  minibatch 24   time 1h 08m 19s   sec/tick 775.6   sec/kimg 48.38   maintenance 0.0    gpumem 5.6\n",
            "tick 6     kimg 96.3     lod 0.00  minibatch 24   time 1h 21m 15s   sec/tick 776.0   sec/kimg 48.40   maintenance 0.0    gpumem 5.6\n",
            "tick 7     kimg 112.3    lod 0.00  minibatch 24   time 1h 34m 11s   sec/tick 776.0   sec/kimg 48.40   maintenance 0.0    gpumem 5.6\n",
            "tick 8     kimg 128.4    lod 0.00  minibatch 24   time 1h 47m 06s   sec/tick 775.5   sec/kimg 48.37   maintenance 0.0    gpumem 5.6\n",
            "tick 9     kimg 144.4    lod 0.00  minibatch 24   time 2h 00m 02s   sec/tick 775.8   sec/kimg 48.39   maintenance 0.0    gpumem 5.6\n",
            "tick 10    kimg 160.4    lod 0.00  minibatch 24   time 2h 12m 59s   sec/tick 776.6   sec/kimg 48.44   maintenance 0.0    gpumem 5.6\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f44472fe000 @  0x7f4be96b4001 0x7f4be619954f 0x7f4be61e9b58 0x7f4be61edb17 0x7f4be628c203 0x593835 0x548c51 0x5127f1 0x549e0e 0x4bca8a 0x532b86 0x53786a 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f4234578000 @  0x7f4be96b21e7 0x7f4be619946e 0x7f4be61e9c7b 0x7f4be61ea35f 0x7f4be628c103 0x593835 0x548c51 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x593dd7 0x5118f8 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x549e0e\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f4234578000 @  0x7f4be96b21e7 0x7f4be619946e 0x7f4be61e9c7b 0x7f4be61ea35f 0x7f4b91164235 0x7f4b90ae7792 0x7f4b90ae7d42 0x7f4b90aa0aee 0x59371f 0x548c51 0x51566f 0x593dd7 0x511e2c 0x549e0e 0x4bcb19 0x5134a6 0x549576 0x593fce 0x511e2c 0x549e0e 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x4bcb19 0x59c019 0x595ef6 0x5134a6 0x549576 0x593fce\n",
            "network-snapshot-000160        time 2m 41s       fid10k 65.9332\n",
            "tick 11    kimg 176.4    lod 0.00  minibatch 24   time 2h 28m 48s   sec/tick 776.2   sec/kimg 48.41   maintenance 173.3  gpumem 5.6\n",
            "tick 12    kimg 192.5    lod 0.00  minibatch 24   time 2h 41m 44s   sec/tick 776.2   sec/kimg 48.42   maintenance 0.0    gpumem 5.6\n",
            "tick 13    kimg 208.5    lod 0.00  minibatch 24   time 2h 54m 40s   sec/tick 776.0   sec/kimg 48.41   maintenance 0.0    gpumem 5.6\n",
            "tick 14    kimg 224.5    lod 0.00  minibatch 24   time 3h 07m 36s   sec/tick 775.7   sec/kimg 48.38   maintenance 0.0    gpumem 5.6\n",
            "tick 15    kimg 240.6    lod 0.00  minibatch 24   time 3h 20m 32s   sec/tick 775.5   sec/kimg 48.37   maintenance 0.0    gpumem 5.6\n",
            "tick 16    kimg 256.6    lod 0.00  minibatch 24   time 3h 33m 27s   sec/tick 775.6   sec/kimg 48.38   maintenance 0.0    gpumem 5.6\n",
            "tick 17    kimg 272.6    lod 0.00  minibatch 24   time 3h 46m 23s   sec/tick 775.6   sec/kimg 48.38   maintenance 0.0    gpumem 5.6\n",
            "tick 18    kimg 288.7    lod 0.00  minibatch 24   time 3h 59m 19s   sec/tick 775.9   sec/kimg 48.39   maintenance 0.0    gpumem 5.6\n",
            "tick 19    kimg 304.7    lod 0.00  minibatch 24   time 4h 12m 15s   sec/tick 776.1   sec/kimg 48.41   maintenance 0.0    gpumem 5.6\n",
            "tick 20    kimg 320.7    lod 0.00  minibatch 24   time 4h 25m 11s   sec/tick 776.0   sec/kimg 48.40   maintenance 0.0    gpumem 5.6\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f4234578000 @  0x7f4be96b4001 0x7f4be619954f 0x7f4be61e9b58 0x7f4be61edb17 0x7f4be628c203 0x593835 0x548c51 0x5127f1 0x549e0e 0x4bca8a 0x532b86 0x53786a 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x5134a6 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f4346afe000 @  0x7f4be96b21e7 0x7f4be619946e 0x7f4be61e9c7b 0x7f4be61ea35f 0x7f4be628c103 0x593835 0x548c51 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x593dd7 0x5118f8 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x549e0e\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x7f4346afe000 @  0x7f4be96b21e7 0x7f4be619946e 0x7f4be61e9c7b 0x7f4be61ea35f 0x7f4b91164235 0x7f4b90ae7792 0x7f4b90ae7d42 0x7f4b90aa0aee 0x59371f 0x548c51 0x51566f 0x593dd7 0x511e2c 0x549e0e 0x4bcb19 0x5134a6 0x549576 0x593fce 0x511e2c 0x549e0e 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x4bcb19 0x59c019 0x595ef6 0x5134a6 0x549576 0x593fce\n",
            "network-snapshot-000320        time 2m 43s       fid10k 40.9591\n",
            "tick 21    kimg 336.8    lod 0.00  minibatch 24   time 4h 41m 02s   sec/tick 776.3   sec/kimg 48.42   maintenance 174.5  gpumem 5.6\n",
            "tick 22    kimg 352.8    lod 0.00  minibatch 24   time 4h 53m 58s   sec/tick 776.4   sec/kimg 48.43   maintenance 0.0    gpumem 5.6\n",
            "tick 23    kimg 368.8    lod 0.00  minibatch 24   time 5h 06m 55s   sec/tick 777.0   sec/kimg 48.47   maintenance 0.0    gpumem 5.6\n",
            "tick 24    kimg 384.9    lod 0.00  minibatch 24   time 5h 19m 51s   sec/tick 776.1   sec/kimg 48.41   maintenance 0.0    gpumem 5.6\n",
            "tick 25    kimg 400.9    lod 0.00  minibatch 24   time 5h 32m 47s   sec/tick 776.2   sec/kimg 48.42   maintenance 0.0    gpumem 5.6\n",
            "tick 26    kimg 416.9    lod 0.00  minibatch 24   time 5h 45m 44s   sec/tick 777.1   sec/kimg 48.47   maintenance 0.0    gpumem 5.6\n",
            "tick 27    kimg 433.0    lod 0.00  minibatch 24   time 5h 58m 41s   sec/tick 776.8   sec/kimg 48.45   maintenance 0.0    gpumem 5.6\n",
            "tick 28    kimg 449.0    lod 0.00  minibatch 24   time 6h 11m 38s   sec/tick 776.2   sec/kimg 48.42   maintenance 0.0    gpumem 5.6\n",
            "tick 29    kimg 465.0    lod 0.00  minibatch 24   time 6h 24m 34s   sec/tick 776.2   sec/kimg 48.42   maintenance 0.0    gpumem 5.6\n",
            "tick 30    kimg 481.1    lod 0.00  minibatch 24   time 6h 37m 30s   sec/tick 776.5   sec/kimg 48.43   maintenance 0.0    gpumem 5.6\n",
            "network-snapshot-000481        time 2m 41s       fid10k 26.6087\n",
            "tick 31    kimg 497.1    lod 0.00  minibatch 24   time 6h 53m 20s   sec/tick 775.9   sec/kimg 48.40   maintenance 173.4  gpumem 5.6\n",
            "tick 32    kimg 513.1    lod 0.00  minibatch 24   time 7h 06m 16s   sec/tick 776.0   sec/kimg 48.41   maintenance 0.0    gpumem 5.6\n",
            "tick 33    kimg 529.2    lod 0.00  minibatch 24   time 7h 19m 11s   sec/tick 775.5   sec/kimg 48.37   maintenance 0.0    gpumem 5.6\n",
            "tick 34    kimg 545.2    lod 0.00  minibatch 24   time 7h 32m 07s   sec/tick 775.7   sec/kimg 48.39   maintenance 0.0    gpumem 5.6\n",
            "tick 35    kimg 561.2    lod 0.00  minibatch 24   time 7h 45m 03s   sec/tick 776.0   sec/kimg 48.40   maintenance 0.0    gpumem 5.6\n",
            "tick 36    kimg 577.2    lod 0.00  minibatch 24   time 7h 57m 59s   sec/tick 775.7   sec/kimg 48.39   maintenance 0.0    gpumem 5.6\n",
            "tick 37    kimg 593.3    lod 0.00  minibatch 24   time 8h 10m 54s   sec/tick 775.0   sec/kimg 48.34   maintenance 0.0    gpumem 5.6\n",
            "tick 38    kimg 609.3    lod 0.00  minibatch 24   time 8h 23m 49s   sec/tick 775.0   sec/kimg 48.34   maintenance 0.0    gpumem 5.6\n",
            "tick 39    kimg 625.3    lod 0.00  minibatch 24   time 8h 36m 44s   sec/tick 775.0   sec/kimg 48.34   maintenance 0.0    gpumem 5.6\n",
            "tick 40    kimg 641.4    lod 0.00  minibatch 24   time 8h 49m 39s   sec/tick 775.0   sec/kimg 48.34   maintenance 0.0    gpumem 5.6\n",
            "network-snapshot-000641        time 2m 42s       fid10k 15.5804\n",
            "tick 41    kimg 657.4    lod 0.00  minibatch 24   time 9h 05m 29s   sec/tick 776.0   sec/kimg 48.40   maintenance 174.3  gpumem 5.6\n",
            "tick 42    kimg 673.4    lod 0.00  minibatch 24   time 9h 18m 28s   sec/tick 779.1   sec/kimg 48.60   maintenance 0.0    gpumem 5.6\n",
            "tick 43    kimg 689.5    lod 0.00  minibatch 24   time 9h 31m 28s   sec/tick 780.2   sec/kimg 48.66   maintenance 0.0    gpumem 5.6\n",
            "tick 44    kimg 705.5    lod 0.00  minibatch 24   time 9h 44m 28s   sec/tick 780.2   sec/kimg 48.66   maintenance 0.0    gpumem 5.6\n",
            "tick 45    kimg 721.5    lod 0.00  minibatch 24   time 9h 57m 28s   sec/tick 779.9   sec/kimg 48.65   maintenance 0.0    gpumem 5.6\n",
            "tick 46    kimg 737.6    lod 0.00  minibatch 24   time 10h 10m 29s  sec/tick 780.4   sec/kimg 48.68   maintenance 0.0    gpumem 5.6\n",
            "tick 47    kimg 753.6    lod 0.00  minibatch 24   time 10h 23m 29s  sec/tick 780.2   sec/kimg 48.66   maintenance 0.0    gpumem 5.6\n",
            "tick 48    kimg 769.6    lod 0.00  minibatch 24   time 10h 36m 29s  sec/tick 780.4   sec/kimg 48.67   maintenance 0.0    gpumem 5.6\n",
            "tick 49    kimg 785.7    lod 0.00  minibatch 24   time 10h 49m 29s  sec/tick 780.1   sec/kimg 48.66   maintenance 0.0    gpumem 5.6\n",
            "tick 50    kimg 801.7    lod 0.00  minibatch 24   time 11h 02m 30s  sec/tick 780.2   sec/kimg 48.66   maintenance 0.0    gpumem 5.6\n",
            "network-snapshot-000801        time 2m 45s       fid10k 11.0411\n",
            "tick 51    kimg 817.7    lod 0.00  minibatch 24   time 11h 18m 28s  sec/tick 780.2   sec/kimg 48.66   maintenance 178.1  gpumem 5.6\n",
            "tick 52    kimg 833.8    lod 0.00  minibatch 24   time 11h 31m 28s  sec/tick 780.4   sec/kimg 48.68   maintenance 0.0    gpumem 5.6\n",
            "tick 53    kimg 849.8    lod 0.00  minibatch 24   time 11h 44m 29s  sec/tick 780.6   sec/kimg 48.69   maintenance 0.0    gpumem 5.6\n",
            "tick 54    kimg 865.8    lod 0.00  minibatch 24   time 11h 57m 29s  sec/tick 780.3   sec/kimg 48.67   maintenance 0.0    gpumem 5.6\n",
            "tick 55    kimg 881.9    lod 0.00  minibatch 24   time 12h 10m 30s  sec/tick 780.3   sec/kimg 48.67   maintenance 0.0    gpumem 5.6\n",
            "tick 56    kimg 897.9    lod 0.00  minibatch 24   time 12h 23m 30s  sec/tick 779.9   sec/kimg 48.65   maintenance 0.0    gpumem 5.6\n",
            "tick 57    kimg 913.9    lod 0.00  minibatch 24   time 12h 36m 28s  sec/tick 778.6   sec/kimg 48.56   maintenance 0.0    gpumem 5.6\n",
            "tick 58    kimg 930.0    lod 0.00  minibatch 24   time 12h 49m 25s  sec/tick 776.6   sec/kimg 48.44   maintenance 0.0    gpumem 5.6\n",
            "tick 59    kimg 946.0    lod 0.00  minibatch 24   time 13h 02m 21s  sec/tick 776.2   sec/kimg 48.42   maintenance 0.0    gpumem 5.6\n",
            "tick 60    kimg 962.0    lod 0.00  minibatch 24   time 13h 15m 16s  sec/tick 775.5   sec/kimg 48.37   maintenance 0.0    gpumem 5.6\n",
            "network-snapshot-000962        time 2m 41s       fid10k 9.0441\n",
            "tick 61    kimg 978.0    lod 0.00  minibatch 24   time 13h 31m 05s  sec/tick 775.5   sec/kimg 48.37   maintenance 173.4  gpumem 5.6\n",
            "tick 62    kimg 994.1    lod 0.00  minibatch 24   time 13h 44m 01s  sec/tick 775.6   sec/kimg 48.38   maintenance 0.0    gpumem 5.6\n",
            "tick 63    kimg 1000.0   lod 0.00  minibatch 24   time 13h 48m 49s  sec/tick 288.0   sec/kimg 48.39   maintenance 0.0    gpumem 5.6\n",
            "network-snapshot-001000        time 2m 40s       fid10k 9.6920\n",
            "network-final                  time 10m 38s      fid50k 8.9761\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python3 run_training.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZStiv4gq-_5s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c434ed8-ae4c-404e-b385-fe7f807700ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.0"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "70/7"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "train_ganformer_mnist_rgbtf_Reproducibility_model_trainer.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}